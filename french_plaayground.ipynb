{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download as nltk_download\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.utils import pad_sequences;\n",
    "from keras.utils import to_categorical;\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "from pickle import dump\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/atakan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/atakan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/atakan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Global variables, constants and some other pre-requisites\n",
    "# Lemmatizer as singleton, set seed for reproducibility\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "RANDOM_SEED = 42\n",
    "DATASET_PATH = \"../en-fr.csv\"\n",
    "DATASET_LENGTH = 100_000\n",
    "ENCODING_LENGTH = 30\n",
    "\n",
    "# Downloading necessary nltk data\n",
    "nltk_download('averaged_perceptron_tagger')\n",
    "nltk_download('wordnet')\n",
    "nltk_download('punkt')\n",
    "\n",
    "# Global Flags\n",
    "DEBUG = True\n",
    "SAVE_DATA = False # Save processed data to disk\n",
    "SHOW_SENTENCE_LENGTS = False # Show the sentence lengths of the dataset, for choosing the ENCODING_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SENTENCE_LENGTS:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Exploratory data analysis to find the ENCODING_LENGTH\n",
    "def add_sentence_length_to_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe with the length of the sentence in words\n",
    "    \"\"\"\n",
    "\n",
    "    def count_words(sentence: str) -> int:\n",
    "        ''' Counts the number of words in a sentence, using vanilla python '''\n",
    "        if isinstance(sentence, float):\n",
    "            return 0\n",
    "        return len(sentence.split())\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df['length'] = df['en'].apply(lambda x: count_words(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_sentence_length(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots the sentence length distribution\n",
    "    \"\"\"\n",
    "    if not SHOW_SENTENCE_LENGTS:\n",
    "        return\n",
    "    \n",
    "    plt.hist(df['length'], bins=200)\n",
    "    plt.xlabel('Sentence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Sentence Length Distribution')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.xticks(np.arange(0, 100, 5))\n",
    "    plt.show()\n",
    "\n",
    "def remove_long_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes sentences that are longer than ENCODING_LENGTH\n",
    "    \"\"\"\n",
    "    # Remove non-string rows (remove ints and floats too)\n",
    "    df = df[df['en'].apply(lambda x: isinstance(x, str))]\n",
    "    df = df[df['fr'].apply(lambda x: isinstance(x, str))]\n",
    "    df['en_len'] = df['en'].apply(lambda x: len(x.split()))\n",
    "    df['fr_len'] = df['fr'].apply(lambda x: len(x.split()))\n",
    "    df = df[df['en_len'] <= ENCODING_LENGTH]\n",
    "    df = df[df['fr_len'] <= ENCODING_LENGTH]\n",
    "    df.drop(['en_len', 'fr_len'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 134220\n",
      "                                                  en  \\\n",
      "0  Changing Lives | Changing Society | How It Wor...   \n",
      "1                                           Site map   \n",
      "2                                           Feedback   \n",
      "3                                            Credits   \n",
      "4                                           Français   \n",
      "5                                    What is light ?   \n",
      "6  The white light spectrum Codes in the light Th...   \n",
      "7  The sky of the first inhabitants A contemporar...   \n",
      "8                                            Cartoon   \n",
      "9                                              Links   \n",
      "\n",
      "                                                  fr  \n",
      "0  Il a transformé notre vie | Il a transformé la...  \n",
      "1                                       Plan du site  \n",
      "2                                        Rétroaction  \n",
      "3                                            Crédits  \n",
      "4                                            English  \n",
      "5                          Qu’est-ce que la lumière?  \n",
      "6  La découverte du spectre de la lumière blanche...  \n",
      "7  Le ciel des premiers habitants La vision conte...  \n",
      "8                                     Bande dessinée  \n",
      "9                                              Liens  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and print some info if in debug mode\n",
    "df = pd.read_csv(DATASET_PATH, nrows=DATASET_LENGTH * 1.3422) # 1.3422 is the magic number to get 100k rows\n",
    "\n",
    "# Remove rows with sentences longer than ENCODING_LENGTH\n",
    "df = remove_long_sentences(df)\n",
    "\n",
    "# Print some info if in debug mode\n",
    "if DEBUG:\n",
    "    if SAVE_DATA:\n",
    "        df.to_csv(f\"en-fr-{DATASET_LENGTH}.csv\", index=False)\n",
    "    if SHOW_SENTENCE_LENGTS:\n",
    "        plot_sentence_length(add_sentence_length_to_df(df))\n",
    "\n",
    "    # Print the number of\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "        \n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and lemmaization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Helper function to convert the pos tag format \n",
    "    into something compatible with the lemmatizer.\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_data(doc: str, expand: bool, lemma: bool):\n",
    "    '''\n",
    "    Cleans the data by removing non alphanumeric characters (except punctuation), \n",
    "    tokenizing and lemmatizing (if specified in the args).\n",
    "\n",
    "    Args:\n",
    "        doc (str): The document to clean\n",
    "        expand (bool): Whether to expand contractions or not\n",
    "        lemma (bool): Whether to lemmatize or not\n",
    "    '''\n",
    "    if expand:\n",
    "        doc = contractions.fix(doc)\n",
    "\n",
    "    # Remove every char that is not alphanumeric or end of sentence punctuation, keep spaces\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^ùûüÿàâæçéèêëïîôœÙÛÜŸÀÂÆÇÉÈÊËÏÎÔŒa-z0-9.!?]+', ' ', doc)\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    if lemma:\n",
    "        pos = pos_tag(tokens)\n",
    "        clean_tokens = [lemmatizer.lemmatize(\n",
    "            word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
    "    else:\n",
    "        clean_tokens = tokens.insert(0,'<bos>')\n",
    "        clean_tokens = clean_tokens.append('<eos>')\n",
    "        \n",
    "    # Add <bos> and <eos> tokens to the sentence\n",
    "    clean_tokens.insert(0, 'beginningOfSentence')\n",
    "    clean_tokens.append('endOfSentence')\n",
    "    \n",
    "    return clean_tokens\n",
    "\n",
    "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Cleans the dataset by applying the clean_data function to each row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to clean\n",
    "    '''\n",
    "    df['en'] = df['en'].apply(lambda x: clean_data(x, expand=True, lemma=True))\n",
    "    df['fr'] = df['fr'].apply(lambda x: clean_data(x, expand=False, lemma=False))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Splits the dataset into train, validation and test sets,\n",
    "    using the split ratio 80-10-10.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to split\n",
    "    '''\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "        df[\"en\"], df[\"fr\"], test_size=0.2, random_state=RANDOM_SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_val_test, y_val_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text: pd.Series , max_words: int = 0):\n",
    "    '''\n",
    "    Creates a tokenizer and fits it on the specified text.\n",
    "\n",
    "    Args:\n",
    "        text (pd.Series): The text to fit the tokenizer on\n",
    "        max_words (int): The maximum number of words to keep (0 means no limit)\n",
    "    '''\n",
    "    if max_words == 0:\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer: Tokenizer, text: pd.Series,pad_len: int = ENCODING_LENGTH):\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): The tokenizer to use \n",
    "        text (pd.Series): The text to encode\n",
    "        pad_len (int): The maximum length of the sequences\n",
    "    '''\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    seq = pad_sequences(seq, maxlen=pad_len, padding='post')\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_encodings(\n",
    "        X : pd.Series,\n",
    "        y: pd.Series, \n",
    "        is_train: bool = False, \n",
    "        maxlen: int = ENCODING_LENGTH,\n",
    "        sc_tokenizer: Tokenizer = None, \n",
    "        tg_tokenizer: Tokenizer = None, \n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Tokenizer, Tokenizer]:\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizers.\n",
    "\n",
    "    Args:\n",
    "        X (pd.Series): The input sequences\n",
    "        y (pd.Series): The target sequences\n",
    "        is_train (bool): Whether to create new tokenizers or not\n",
    "        maxlen (int): The maximum length of the sequences\n",
    "        sc_tokenizer (Tokenizer): The source language tokenizer\n",
    "        tg_tokenizer (Tokenizer): The target language tokenizer\n",
    "    '''\n",
    "    # Only create and fit a new tokenizer on the training set\n",
    "    if is_train:\n",
    "        sc_tokenizer = create_tokenizer(X)\n",
    "        tg_tokenizer = create_tokenizer(y)\n",
    "\n",
    "    X_encoded = encode_sequences(sc_tokenizer, X, maxlen)\n",
    "    y_encoded = encode_sequences(tg_tokenizer, y, maxlen)\n",
    "\n",
    "    return X_encoded, y_encoded, sc_tokenizer, tg_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, is_clean: bool = False):\n",
    "    '''\n",
    "    Preprocesses the dataset by cleaning it and splitting it into train, validation and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to preprocess\n",
    "        is_clean (bool): Whether to load a clean version of the dataset or not\n",
    "    '''\n",
    "    if not is_clean:\n",
    "        df = df.dropna()\n",
    "        df_clean = clean(df)\n",
    "        df_clean = df_clean.dropna()\n",
    "        df_clean.to_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "    else:\n",
    "        df_clean = pd.read_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "        \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split(df_clean)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING\n",
    "\n",
    "Here we apply the functions we have defined above to pre-process the data. The pre-processing steps are:\n",
    "\n",
    "- Cleaning the data\n",
    "    - Remove every char that is not alphanumeric or end of sentence punctuation, keep spaces\n",
    "    - Lemmatization\n",
    "- Tokenization\n",
    "- Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing to the dataset\n",
    "# In a separate cell to avoid re-running it every time (takes a while)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess(df, is_clean=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48930\n",
      "64880\n",
      "Second row of the training set: [   3  626  242  207    8  225   23  818 1744    9    1 1515  158 1220\n",
      "    8  176 1916    7  692   42  932    9    1   35 2758    2    4    0\n",
      "    0    0]\n",
      "The corrresponding sentence: ['beginningOfSentence', 'detail', 'search', 'help', 'be', 'available', 'from', 'every', 'screen', 'in', 'the', 'catalogue', 'but', 'here', 'be', 'some', 'tip', 'to', 'get', 'you', 'start', 'in', 'the', 'new', 'interface', '.', 'endOfSentence']\n"
     ]
    }
   ],
   "source": [
    " # Turn sentences into tokenized and padded sequences\n",
    "X_train_encoded, y_train_encoded, en_tokenizer, fr_tokenizer = get_encodings(X_train, y_train, is_train=True)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(en_tokenizer.word_index) + 1)\n",
    "    print(len(fr_tokenizer.word_index) + 1)\n",
    "    print(f\"Second row of the training set: {X_train_encoded[6]}\")\n",
    "    print(f\"The corrresponding sentence: {X_train.iloc[6]}\")\n",
    "\n",
    "X_val_encoded, y_val_encoded, _, _ = get_encodings(X=X_val, is_train=False, y=y_val, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)\n",
    "\n",
    "X_test_encoded, y_test_encoded, _, _ = get_encodings(X_test, y_test, is_train=False, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)\n",
    "\n",
    "with open('../fr_train_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f: \n",
    "            np.save(f, X_train_encoded)\n",
    "            np.save(f, y_train_encoded)\n",
    "\n",
    "with open('../fr_test_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f:\n",
    "            np.save(f, X_test_encoded)\n",
    "            np.save(f, y_test_encoded)\n",
    "\n",
    "with open('../fr_valid_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f:\n",
    "            np.save(f, X_val_encoded)\n",
    "            np.save(f, y_val_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL, TRAINING AND PREDICTION\n",
    "\n",
    "Here we define our model, the training and prediction functions. Then, we train/load the model and predict the selected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare model hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "PATIENCE = 5\n",
    "LOAD_MODEL = False\n",
    "GRU_UNITS = 256\n",
    "LEARNING_RATE = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Embedding, Bidirectional, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def define_model(\n",
    "        in_vocab_size: int, \n",
    "        out_vocab_size: int, \n",
    "        in_seq_length: int = ENCODING_LENGTH, \n",
    "        out_seq_length: int = ENCODING_LENGTH, \n",
    "        embedding_size: int = 50\n",
    "    ) -> Sequential:\n",
    "    '''\n",
    "    Defines the model architecture.\n",
    "\n",
    "    Args:\n",
    "        in_vocab_size (int): The size of the source language vocabulary\n",
    "        out_vocab_size (int): The size of the target language vocabulary\n",
    "        in_seq_length (int): The maximum length of the source language sequences\n",
    "        out_seq_length (int): The maximum length of the target language sequences\n",
    "        embedding_size (int): The size of the embedding layer\n",
    "    '''\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding\n",
    "    model.add(Embedding(input_dim=in_vocab_size,\n",
    "              output_dim=embedding_size, input_length=in_seq_length))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(GRU_UNITS)))\n",
    "    # Decoder\n",
    "    model.add(RepeatVector(out_seq_length))\n",
    "    # GRU\n",
    "    model.add(GRU(GRU_UNITS, return_sequences=True))\n",
    "    # Prediction\n",
    "    model.add(TimeDistributed(Dense(out_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=RMSprop(lr=LEARNING_RATE),\n",
    "                  metrics=\"accuracy\")\n",
    "    \n",
    "    if DEBUG:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: Sequential, \n",
    "        X_train: np.ndarray, \n",
    "        y_train: np.ndarray, \n",
    "        X_val: np.ndarray, \n",
    "        y_val: np.ndarray\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): The model to train\n",
    "        X_train (np.ndarray): The training set\n",
    "        y_train (np.ndarray): The training labels\n",
    "        X_val (np.ndarray): The validation set\n",
    "        y_val (np.ndarray): The validation labels\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "        validation_data=(X_val, y_val),verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    model.save('../fr-model.h5')\n",
    "    return history\n",
    "\n",
    "\n",
    "def vector_to_word(embedding: np.ndarray, tokenizer: Tokenizer):\n",
    "    '''\n",
    "    Converts a vectorized word back to its original form.\n",
    "\n",
    "    Args:\n",
    "        embedding (np.ndarray): The vectorized word\n",
    "        tokenizer (Tokenizer): The tokenizer to use\n",
    "    '''\n",
    "    idx = np.argmax(embedding)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(idx)\n",
    "    for word, transform in tokenizer.word_index.items():\n",
    "        if transform == idx:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentences(sequences, tokenizer: Tokenizer):\n",
    "    '''\n",
    "    Converts a list of sequences to a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): The list of sequences\n",
    "        tokenizer (Tokenizer): The tokenizer to use\n",
    "    '''\n",
    "    predictions = []\n",
    "    for sentence in sequences:\n",
    "        predict = ''\n",
    "        for emb in sentence:\n",
    "            word = vector_to_word(emb, tokenizer)\n",
    "            if word is not None:\n",
    "                predict += word + ' '\n",
    "        predictions.append(predict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "if LOAD_MODEL:\n",
    "    from keras.models import load_model\n",
    "    model = load_model(\"../fr-model-csanad.h5\")\n",
    "else:\n",
    "    en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "    fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "    model = define_model(en_vocab_size, fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'keras.engine.sequential.Sequential'>\n",
      "X_train type: <class 'numpy.ndarray'>\n",
      "y_train type: <class 'numpy.ndarray'>\n",
      "X_val type: <class 'numpy.ndarray'>\n",
      "y_val type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 20:26:49.527290: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:49.529397: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:49.531037: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:49.798328: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-16 20:26:49.883982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:49.889572: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:49.894378: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:50.325235: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:50.329720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:50.331654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:52.549702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-16 20:26:53.317605: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:53.325084: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:53.327577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:53.701494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-16 20:26:53.833698: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:53.840872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:53.845792: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:54.462944: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-16 20:26:54.470153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-16 20:26:54.475381: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-16 20:26:56.447220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = train(model, X_train_encoded, y_train_encoded, X_val_encoded, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the first 100 training examples\n",
    "prediction = model.predict(X_train_encoded[0:100])\n",
    "prediction_sentences = get_sentences(prediction, fr_tokenizer)\n",
    "candidate_translations = [[sentence] for sentence in prediction_sentences]\n",
    "print(candidate_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model.save('../fr-model-Atakan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer (didn't remove this, might be useful to at least someone)\n",
    "from pickle import dump\n",
    "\n",
    "dump(fr_tokenizer, open(\"../fr_tokenizer.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-nlp",
   "language": "python",
   "name": ".venv-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
