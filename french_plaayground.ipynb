{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download as nltk_download\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.utils import pad_sequences;\n",
    "from keras.utils import to_categorical;\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "from pickle import dump\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/atakan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/atakan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/atakan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Global variables, constants and some other pre-requisites\n",
    "# Lemmatizer as singleton, set seed for reproducibility\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "RANDOM_SEED = 42\n",
    "DATASET_PATH = \"../en-fr.csv\"\n",
    "DATASET_LENGTH = 100_000\n",
    "ENCODING_LENGTH = 30\n",
    "\n",
    "# Downloading necessary nltk data\n",
    "nltk_download('averaged_perceptron_tagger')\n",
    "nltk_download('wordnet')\n",
    "nltk_download('punkt')\n",
    "\n",
    "# Global Flags\n",
    "DEBUG = True\n",
    "SAVE_DATA = False # Save processed data to disk\n",
    "SHOW_SENTENCE_LENGTS = False # Show the sentence lengths of the dataset, for choosing the ENCODING_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SENTENCE_LENGTS:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Exploratory data analysis to find the ENCODING_LENGTH\n",
    "def add_sentence_length_to_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column to the dataframe with the length of the sentence in words\n",
    "    \"\"\"\n",
    "\n",
    "    def count_words(sentence: str) -> int:\n",
    "        ''' Counts the number of words in a sentence, using vanilla python '''\n",
    "        if isinstance(sentence, float):\n",
    "            return 0\n",
    "        return len(sentence.split())\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df['length'] = df['en'].apply(lambda x: count_words(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_sentence_length(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots the sentence length distribution\n",
    "    \"\"\"\n",
    "    if not SHOW_SENTENCE_LENGTS:\n",
    "        return\n",
    "    \n",
    "    plt.hist(df['length'], bins=200)\n",
    "    plt.xlabel('Sentence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Sentence Length Distribution')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.xticks(np.arange(0, 100, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  en  \\\n",
      "0  Changing Lives | Changing Society | How It Wor...   \n",
      "1                                           Site map   \n",
      "2                                           Feedback   \n",
      "3                                            Credits   \n",
      "4                                           Français   \n",
      "5                                    What is light ?   \n",
      "6  The white light spectrum Codes in the light Th...   \n",
      "7  The sky of the first inhabitants A contemporar...   \n",
      "8                                            Cartoon   \n",
      "9                                              Links   \n",
      "\n",
      "                                                  fr  \n",
      "0  Il a transformé notre vie | Il a transformé la...  \n",
      "1                                       Plan du site  \n",
      "2                                        Rétroaction  \n",
      "3                                            Crédits  \n",
      "4                                            English  \n",
      "5                          Qu’est-ce que la lumière?  \n",
      "6  La découverte du spectre de la lumière blanche...  \n",
      "7  Le ciel des premiers habitants La vision conte...  \n",
      "8                                     Bande dessinée  \n",
      "9                                              Liens  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and print some info if in debug mode\n",
    "df = pd.read_csv(DATASET_PATH, nrows=DATASET_LENGTH)\n",
    "\n",
    "if DEBUG:\n",
    "    if SAVE_DATA:\n",
    "        df.to_csv(f\"en-fr-{DATASET_LENGTH}.csv\", index=False)\n",
    "    if SHOW_SENTENCE_LENGTS:\n",
    "        plot_sentence_length(add_sentence_length_to_df(df))\n",
    "        \n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and lemmaization\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Helper function to convert the pos tag format \n",
    "    into something compatible with the lemmatizer.\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_data(doc: str, expand: bool, lemma: bool):\n",
    "    '''\n",
    "    Cleans the data by removing non alphanumeric characters (except punctuation), \n",
    "    tokenizing and lemmatizing (if specified in the args).\n",
    "\n",
    "    Args:\n",
    "        doc (str): The document to clean\n",
    "        expand (bool): Whether to expand contractions or not\n",
    "        lemma (bool): Whether to lemmatize or not\n",
    "    '''\n",
    "    if expand:\n",
    "        doc = contractions.fix(doc)\n",
    "\n",
    "    # Remove every char that is not alphanumeric or end of sentence punctuation, keep spaces\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^ùûüÿàâæçéèêëïîôœÙÛÜŸÀÂÆÇÉÈÊËÏÎÔŒa-z0-9.!?]+', ' ', doc)\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    if lemma:\n",
    "        pos = pos_tag(tokens)\n",
    "        clean_tokens = [lemmatizer.lemmatize(\n",
    "            word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
    "    else:\n",
    "        clean_tokens = tokens\n",
    "        \n",
    "    return clean_tokens\n",
    "\n",
    "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Cleans the dataset by applying the clean_data function to each row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to clean\n",
    "    '''\n",
    "    df['en'] = df['en'].apply(lambda x: clean_data(x, expand=True, lemma=True))\n",
    "    df['fr'] = df['fr'].apply(lambda x: clean_data(x, expand=False, lemma=False))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Splits the dataset into train, validation and test sets,\n",
    "    using the split ratio 80-10-10.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to split\n",
    "    '''\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "        df[\"en\"], df[\"fr\"], test_size=0.2, random_state=RANDOM_SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_val_test, y_val_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text: pd.Series , max_words: int = 0):\n",
    "    '''\n",
    "    Creates a tokenizer and fits it on the specified text.\n",
    "\n",
    "    Args:\n",
    "        text (pd.Series): The text to fit the tokenizer on\n",
    "        max_words (int): The maximum number of words to keep (0 means no limit)\n",
    "    '''\n",
    "    if max_words == 0:\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer: Tokenizer, text: pd.Series, pad_len: int = ENCODING_LENGTH):\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): The tokenizer to use \n",
    "        text (pd.Series): The text to encode\n",
    "        pad_len (int): The maximum length of the sequences\n",
    "    '''\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    seq = pad_sequences(seq, maxlen=pad_len, padding='post')\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_encodings(\n",
    "        X : pd.Series,\n",
    "        y: pd.Series = None, \n",
    "        is_train: bool =False, \n",
    "        maxlen: int = ENCODING_LENGTH,\n",
    "        sc_tokenizer: Tokenizer = None, \n",
    "        tg_tokenizer: Tokenizer = None, \n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Tokenizer, Tokenizer]:\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizers.\n",
    "\n",
    "    Args:\n",
    "        X (pd.Series): The input sequences\n",
    "        y (pd.Series): The target sequences\n",
    "        is_train (bool): Whether to create new tokenizers or not\n",
    "        maxlen (int): The maximum length of the sequences\n",
    "        sc_tokenizer (Tokenizer): The source language tokenizer\n",
    "        tg_tokenizer (Tokenizer): The target language tokenizer\n",
    "    '''\n",
    "    # Only create and fit a new tokenizer on the training set\n",
    "    if is_train:\n",
    "        sc_tokenizer = create_tokenizer(X)\n",
    "        tg_tokenizer = create_tokenizer(y)\n",
    "\n",
    "    X_encoded = encode_sequences(sc_tokenizer, X, maxlen)\n",
    "    y_encoded = encode_sequences(tg_tokenizer, y, maxlen)\n",
    "\n",
    "    return X_encoded, y_encoded, sc_tokenizer, tg_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, is_clean: bool = False):\n",
    "    '''\n",
    "    Preprocesses the dataset by cleaning it and splitting it into train, validation and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to preprocess\n",
    "        is_clean (bool): Whether to load a clean version of the dataset or not\n",
    "    '''\n",
    "    if not is_clean:\n",
    "        df = df.dropna()\n",
    "        df_clean = clean(df)\n",
    "        df_clean = df_clean.dropna()\n",
    "        df_clean.to_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "    else:\n",
    "        df_clean = pd.read_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "        \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split(df_clean)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING\n",
    "\n",
    "Here we apply the functions we have defined above to pre-process the data. The pre-processing steps are:\n",
    "\n",
    "- Cleaning the data\n",
    "    - Remove every char that is not alphanumeric or end of sentence punctuation, keep spaces\n",
    "    - Lemmatization\n",
    "- Tokenization\n",
    "- Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing to the dataset\n",
    "# In a separate cell to avoid re-running it every time (takes a while)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess(df, is_clean=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42829\n",
      "57810\n"
     ]
    }
   ],
   "source": [
    " # Turn sentences into tokenized and padded sequences\n",
    "X_train_encoded, y_train_encoded, en_tokenizer, fr_tokenizer = get_encodings(X_train, y_train, is_train=True)\n",
    "\n",
    "if DEBUG:\n",
    "    print(len(en_tokenizer.word_index) + 1)\n",
    "    print(len(fr_tokenizer.word_index) + 1)\n",
    "\n",
    "X_val_encoded, y_val_encoded, _, _ = get_encodings(X=X_val, is_train=False, y=y_val, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)\n",
    "\n",
    "X_test_encoded, y_test_encoded, _, _ = get_encodings(X_test, y_test, is_train=False, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)\n",
    "\n",
    "with open('../fr_train_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f: \n",
    "            np.save(f, X_train_encoded)\n",
    "            np.save(f, y_train_encoded)\n",
    "\n",
    "with open('../fr_test_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f:\n",
    "            np.save(f, X_test_encoded)\n",
    "            np.save(f, y_test_encoded)\n",
    "\n",
    "with open('../fr_valid_data' + str(int(DATASET_LENGTH/1_000)) + '.npy', 'wb') as f:\n",
    "            np.save(f, X_val_encoded)\n",
    "            np.save(f, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Embedding, Bidirectional, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def define_model(in_vocab_size, out_vocab_size, embedding_matrix=None, in_seq_length=40, out_seq_length=40, embedding_size=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=in_vocab_size,\n",
    "              output_dim=embedding_size, input_length=in_seq_length))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(256)))\n",
    "    # Decoder\n",
    "    model.add(RepeatVector(out_seq_length))\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    # Prediction\n",
    "    model.add(TimeDistributed(Dense(out_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.003),\n",
    "                  metrics=\"accuracy\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_val, y_val):\n",
    "    history = model.fit(X_train, y_train, batch_size=128, epochs=1, validation_data=(X_val, y_val),\n",
    "              verbose=1,\n",
    "              callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ])\n",
    "    model.save('../fr-model.h5')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "print(en_vocab_size, fr_vocab_size)\n",
    "#model = define_model(en_vocab_size, fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model, X_train_encoded, y_train_encoded, X_val_encoded, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_word(embedding, tokenizer):\n",
    "    idx = np.argmax(embedding)\n",
    "    print(idx)\n",
    "    for word, transform in tokenizer.word_index.items():\n",
    "        if transform == idx:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentences(sequences, tokenizer):\n",
    "    predictions = []\n",
    "    for sentence in sequences:\n",
    "        predict = ''\n",
    "        for emb in sentence:\n",
    "            word = vector_to_word(emb, tokenizer)\n",
    "            if word is not None:\n",
    "                predict += word + ' '\n",
    "        predictions.append(predict)\n",
    "    return predictions\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(\"../fr-model-csanad.h5\")\n",
    "prediction = model.predict(X_train_encoded[0:100])\n",
    "prediction_sentences = get_sentences(prediction, fr_tokenizer)\n",
    "candidate_translations = [[sentence] for sentence in prediction_sentences]\n",
    "print(candidate_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "dump(fr_tokenizer, open(\"../fr_tokenizer.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
