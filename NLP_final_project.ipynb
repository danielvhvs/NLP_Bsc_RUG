{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "### Team 3: Daniel van Heuven van Staereling, Teo Stereciu, Csanad Vegh, Atakan Tekparmak\n",
    "\n",
    "For the final project we tackled a machine translation task from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.utils import pad_sequences;\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables, constants\n",
    "# Set seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "DATASET_PATH = \"../en-fr.csv\"\n",
    "DATASET_LENGTH = 50_000\n",
    "ENCODING_LENGTH = 35\n",
    "\n",
    "# Global Flags\n",
    "DEBUG = True\n",
    "SAVE_DATA = False # Save clean data to disk\n",
    "SHOW_SENTENCE_LENGHTS = True # Show the sentence lengths of the dataset, for choosing the ENCODING_LENGTH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING\n",
    "\n",
    "Here we preprocess the data following the steps described below:\n",
    "\n",
    "- Cleaning the data\n",
    "    - Expand English contractions\n",
    "    - Remove every char that is not alphanumeric or end of sentence punctuation, keep spaces and French accents\n",
    "    - Lowercase\n",
    "    - Add bos and eos tags\n",
    "- Tokenization\n",
    "- Encoding into sequences of integers, where each integer is the index of a word from the vocabulary\n",
    "- Padding of the zequences with zeros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(doc: str, expand: bool, lemma: bool):\n",
    "    '''\n",
    "    Cleans the data by expanding contractions (if specified in args),\n",
    "    removing non alphanumeric characters (except end of sentence punctuation), \n",
    "    lowercasing,\n",
    "    and adding bos and eos tags.\n",
    "\n",
    "    Args:\n",
    "        doc (str): The document to clean\n",
    "        expand (bool): Whether to expand contractions or not\n",
    "    '''\n",
    "    if expand:\n",
    "        doc = contractions.fix(doc)\n",
    "\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^ùûüÿàâæçéèêëïîôœÙÛÜŸÀÂÆÇÉÈÊËÏÎÔŒa-z0-9.!?]+', ' ', doc)\n",
    "    doc = 'bos ' + doc + ' eos'\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Cleans the dataset by applying the clean_data function to each row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to clean\n",
    "    '''\n",
    "    df['en'] = df['en'].apply(lambda x: clean_data(x, expand=True, lemma=True))\n",
    "    df['fr'] = df['fr'].apply(lambda x: clean_data(x, expand=False, lemma=False))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Splits the dataset into train, validation and test sets,\n",
    "    using the split ratio 80-10-10.\n",
    "        \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to split\n",
    "    '''\n",
    "    X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "        df[\"en\"], df[\"fr\"], test_size=0.2, random_state=RANDOM_SEED)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_val_test, y_val_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, is_clean: bool = False):\n",
    "    '''\n",
    "    Preprocesses the dataset by cleaning it and splitting it into train, validation and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to preprocess\n",
    "        is_clean (bool): Whether to load a clean version of the dataset or apply cleaning\n",
    "    '''\n",
    "    if not is_clean:\n",
    "        df = df.dropna()\n",
    "        df_clean = clean(df)\n",
    "        df_clean = df_clean.dropna()\n",
    "        df_clean.to_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "    else:\n",
    "        df_clean = pd.read_pickle('../fr-clean-data' + str(int(DATASET_LENGTH/1_000)) + '.pkl')\n",
    "        \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split(df_clean)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH, nrows=DATASET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/dl1z_tcs7zb6pppfbf65d5sh0000gn/T/ipykernel_76974/925232819.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['en'] = df['en'].apply(lambda x: clean_data(x, expand=True, lemma=True))\n",
      "/var/folders/x1/dl1z_tcs7zb6pppfbf65d5sh0000gn/T/ipykernel_76974/925232819.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['fr'] = df['fr'].apply(lambda x: clean_data(x, expand=False, lemma=False))\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess(df, is_clean=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is clean and split, let us look at what the sentence length tends to be for English and French. We see that the length 35 would cover most of the corpus in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sent_len(lenghts, lang):\n",
    "    plt.hist(lenghts, bins=200)\n",
    "    plt.xlabel('Sentence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(lang + ' Sentence Length Distribution')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.xticks(np.arange(0, 100, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SENTENCE_LENGHTS:\n",
    "    en_sen_lens = [len(sentence.split()) for sentence in X_train]\n",
    "    fr_sen_lens = [len(sentence.split()) for sentence in y_train]\n",
    "    plot_sent_len(en_sen_lens, \"English\")\n",
    "    plot_sent_len(fr_sen_lens, \"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(text: pd.Series , max_words: int = 0):\n",
    "    '''\n",
    "    Creates a tokenizer and fits it on the specified text.\n",
    "\n",
    "    Args:\n",
    "        text (pd.Series): The text to fit the tokenizer on\n",
    "        max_words (int): The maximum number of words to keep (0 means no limit)\n",
    "    '''\n",
    "    if max_words == 0:\n",
    "        tokenizer = Tokenizer()\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_sequences(tokenizer: Tokenizer, text: pd.Series, pad_len: int = ENCODING_LENGTH):\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): The tokenizer to use \n",
    "        text (pd.Series): The text to encode\n",
    "        pad_len (int): The maximum length of the sequences\n",
    "    '''\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    seq = pad_sequences(seq, maxlen=pad_len, padding='post')\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_encodings(\n",
    "        X : pd.Series,\n",
    "        y: pd.Series, \n",
    "        is_train: bool = False, \n",
    "        maxlen: int = ENCODING_LENGTH,\n",
    "        sc_tokenizer: Tokenizer = None, \n",
    "        tg_tokenizer: Tokenizer = None, \n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Tokenizer, Tokenizer]:\n",
    "    '''\n",
    "    Encodes the sequences using the specified tokenizers.\n",
    "\n",
    "    Args:\n",
    "        X (pd.Series): The input sequences\n",
    "        y (pd.Series): The target sequences\n",
    "        is_train (bool): Whether to create new tokenizers or not\n",
    "        maxlen (int): The maximum length of the sequences\n",
    "        sc_tokenizer (Tokenizer): The source language tokenizer\n",
    "        tg_tokenizer (Tokenizer): The target language tokenizer\n",
    "    '''\n",
    "    # Only create and fit a new tokenizer on the training set\n",
    "    if is_train:\n",
    "        sc_tokenizer = create_tokenizer(X)\n",
    "        tg_tokenizer = create_tokenizer(y)\n",
    "\n",
    "    X_encoded = encode_sequences(sc_tokenizer, X, maxlen)\n",
    "    y_encoded = encode_sequences(tg_tokenizer, y, maxlen)\n",
    "\n",
    "    return X_encoded, y_encoded, sc_tokenizer, tg_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Turn sentences into tokenized and padded sequences\n",
    "X_train_encoded, y_train_encoded, en_tokenizer, fr_tokenizer = get_encodings(X_train, y_train, is_train=True)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"English vocabulary size: {len(en_tokenizer.word_index) + 1}\")\n",
    "    print(f\"French vocabulary size: {len(fr_tokenizer.word_index) + 1}\")\n",
    "    print(f\"An encoded row of the English training set: {X_train_encoded[860]}\")\n",
    "    print(f\"The corrresponding sentence: {X_train.iloc[860]}\")\n",
    "    print(f\"And its French translation: {y_train.iloc[860]}\")\n",
    "\n",
    "X_val_encoded, y_val_encoded, _, _ = get_encodings(X_val, y_val, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)\n",
    "\n",
    "X_test_encoded, y_test_encoded, _, _ = get_encodings(X_test, y_test, sc_tokenizer=en_tokenizer, tg_tokenizer=fr_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "Here we define our model, the training and prediction functions. Then, we train/load the model and evaluate it on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare model hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "UNITS = 256\n",
    "\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def define_model(\n",
    "        in_vocab_size: int, \n",
    "        out_vocab_size: int, \n",
    "        in_seq_length: int = ENCODING_LENGTH, \n",
    "        out_seq_length: int = ENCODING_LENGTH, \n",
    "        units: int = UNITS\n",
    "    ) -> Sequential:\n",
    "    '''\n",
    "    Defines the model architecture.\n",
    "\n",
    "    Args:\n",
    "        in_vocab_size (int): The size of the source language vocabulary\n",
    "        out_vocab_size (int): The size of the target language vocabulary\n",
    "        in_seq_length (int): The maximum length of the source language sequences\n",
    "        out_seq_length (int): The maximum length of the target language sequences\n",
    "        units (int): Number of neurons in each layer\n",
    "    '''\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding\n",
    "    model.add(Embedding(input_dim=in_vocab_size,\n",
    "              output_dim=units, input_length=in_seq_length, mask_zero=True))\n",
    "    # Encoder\n",
    "    model.add(LSTM(units))\n",
    "    # Decoder\n",
    "    model.add(RepeatVector(out_seq_length))\n",
    "    # GRU\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    # Prediction\n",
    "    model.add(TimeDistributed(Dense(out_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=\"RMSprop\",\n",
    "                  metrics=\"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: Sequential, \n",
    "        X_train: np.ndarray, \n",
    "        y_train: np.ndarray, \n",
    "        X_val: np.ndarray, \n",
    "        y_val: np.ndarray\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Trains the model.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): The model to train\n",
    "        X_train (np.ndarray): The source language sequences from the training set\n",
    "        y_train (np.ndarray): The target language sequences from the training set\n",
    "        X_val (np.ndarray): The source language sequences from the validation set\n",
    "        y_val (np.ndarray): The target language sequences from the validation set\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "        validation_data=(X_val, y_val), verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    if SAVE_MODEL:\n",
    "        model.save('../fr-model.h5')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "if LOAD_MODEL:\n",
    "    from keras.models import load_model\n",
    "    from pickle import load\n",
    "    fr_tokenizer = load(open(\"../fr_tokenizer.pkl\", \"rb\"))\n",
    "    model = load_model(\"../fr-model.h5\")\n",
    "    \n",
    "else:\n",
    "    en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "    fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "    model = define_model(en_vocab_size, fr_vocab_size)\n",
    "    if SAVE_MODEL:\n",
    "        from pickle import dump\n",
    "        dump(fr_tokenizer, open(\"../fr_tokenizer.pkl\", \"wb\"))\n",
    "\n",
    "if DEBUG:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train(model, X_train_encoded, y_train_encoded, X_val_encoded, y_val_encoded)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_word(prediction: np.ndarray, tokenizer: Tokenizer):\n",
    "    '''\n",
    "    Converts the most likely word index in the prediction back to text.\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): A probability distribution over the vocabulary\n",
    "        tokenizer (Tokenizer): The tokenizer to use\n",
    "    '''\n",
    "    idx = np.argmax(prediction)\n",
    "\n",
    "    for word, transform in tokenizer.word_index.items():\n",
    "        if transform == idx:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentences(sequences, tokenizer: Tokenizer):\n",
    "    '''\n",
    "    Converts a list of sequences to a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): The prediction of the model\n",
    "        tokenizer (Tokenizer): The tokenizer to use\n",
    "    '''\n",
    "    predictions = []\n",
    "    for sentence in sequences:\n",
    "        predict = ''\n",
    "        for word_pred in sentence:\n",
    "            word = vector_to_word(word_pred, tokenizer)\n",
    "            if word is not None:\n",
    "                predict += word + ' '\n",
    "        predictions.append(predict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test_encoded)\n",
    "prediction_sentences = get_sentences(prediction, fr_tokenizer)\n",
    "candidate_translations = [[sentence] for sentence in prediction_sentences]\n",
    "print(candidate_translations)\n",
    "# todo make this nicer and add bleu score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-nlp",
   "language": "python",
   "name": ".venv-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
